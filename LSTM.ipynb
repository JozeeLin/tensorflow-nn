{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "oH00EWA1bhrz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "循环神经网络由于神经网络结构的进步和GPU上深度学习训练效率的突破，RNN变得越来越流行。RNN对时间序列数据非常有效，其每个神经元可通过内部组件保存之前输入的信息。\n",
        "\n",
        "人每次思考时不会从头开始，而是保留之前思考的一些结果为现在的决策提供支持。例如我们对话时，会根据上下文的信息理解一句话的含义，而不是对每一句话从头进行分析。\n",
        "\n",
        "例如卷积神经网络虽然可以对图像进行分类，但是可能无法对视频中每一帧图像发生的事情进行关联分析，我们无法利用前一帧图像的信息，而循环神经网络则可以解决这个问题。\n",
        "\n",
        "RNN最大特点是神经元的某些输出可作为其输入再次传输到神经元中，因此可以利用之前的信息。\n",
        "\n",
        "RNN虽然被设计成可以处理整个时间序列信息，但是其记忆最深的还是最后输入的一些信号。而更早之前的信号的强度则越来越低，最后只能起到一点辅助的作用，即决定RNN输出的还是最后输入的一些信号。\n",
        "\n",
        "对于某些简单的问题，可能只需要最后输入的少量时序信息即可解决。但对某些复杂的问题，可能需要更早的一些信息，甚至是时间序列开头的信息，但间隔太远的输入信息，RNN是难以记忆的。因此长程依赖是传统RNN的致命伤。\n",
        "\n",
        "## LSTM\n",
        "包含4层神经网络\n",
        "\n",
        "## 语言模型\n",
        "\n",
        "语言模型是NLP中非常重要的一个部分，同时也是语音识别、机器翻译和由图片生成标题等任务的基础和关键。**语言模型是一个可以预测语言的概率模型**。给定上文的语境，即历史出现的单词，语言模型可以预测下一个单词出现的频率。\n",
        "\n",
        "**Penn Tree Bank(PTB)是在语言模型训练中经常使用的一个数据集**，它的质量比较高，可以用来评测语言模型的准确率，同时数据集不大，训练也比较快。参考论文[Recurrent Neural Network Regularization](https://arxiv.org/pdf/1409.2329.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "vG8cY2Cqbb9S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "outputId": "d3987a14-22b0-4d5a-8768-8a0dcc17d06e"
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/tensorflow/models.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'models'...\n",
            "remote: Counting objects: 18265, done.\u001b[K\n",
            "remote: Compressing objects: 100% (35/35), done.\u001b[K\n",
            "remote: Total 18265 (delta 14), reused 16 (delta 6), pack-reused 18224\u001b[K\n",
            "Receiving objects: 100% (18265/18265), 470.57 MiB | 20.66 MiB/s, done.\n",
            "Resolving deltas: 100% (10774/10774), done.\n",
            "Checking out files: 100% (2364/2364), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gVOGRTlIbkrY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('models/tutorials/rnn/ptb')\n",
        "import time \n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import reader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t8a-KdFfbl1S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1654
        },
        "outputId": "2e27f6a7-ac7a-458e-bf13-efb256ee3ec9"
      },
      "cell_type": "code",
      "source": [
        "#下载PTB数据集，并解压\n",
        "!wget http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
        "!tar xvf simple-examples.tgz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-06-24 08:34:49--  http://www.fit.vutbr.cz/~imikolov/rnnlm/simple-examples.tgz\n",
            "Resolving www.fit.vutbr.cz (www.fit.vutbr.cz)... 147.229.9.23, 2001:67c:1220:809::93e5:917\n",
            "Connecting to www.fit.vutbr.cz (www.fit.vutbr.cz)|147.229.9.23|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 34869662 (33M) [application/x-gtar]\n",
            "Saving to: ‘simple-examples.tgz’\n",
            "\n",
            "simple-examples.tgz 100%[===================>]  33.25M  3.82MB/s    in 9.3s    \n",
            "\n",
            "2018-06-24 08:34:59 (3.57 MB/s) - ‘simple-examples.tgz’ saved [34869662/34869662]\n",
            "\n",
            "./\n",
            "./simple-examples/\n",
            "./simple-examples/data/\n",
            "./simple-examples/data/ptb.test.txt\n",
            "./simple-examples/data/ptb.train.txt\n",
            "./simple-examples/data/ptb.valid.txt\n",
            "./simple-examples/data/README\n",
            "./simple-examples/data/ptb.char.train.txt\n",
            "./simple-examples/data/ptb.char.test.txt\n",
            "./simple-examples/data/ptb.char.valid.txt\n",
            "./simple-examples/models/\n",
            "./simple-examples/models/swb.ngram.model\n",
            "./simple-examples/models/swb.rnn.model\n",
            "./simple-examples/models/README\n",
            "./simple-examples/rnnlm-0.2b/\n",
            "./simple-examples/rnnlm-0.2b/CHANGE.log\n",
            "./simple-examples/rnnlm-0.2b/FAQ.txt\n",
            "./simple-examples/rnnlm-0.2b/convert.c\n",
            "./simple-examples/rnnlm-0.2b/makefile\n",
            "./simple-examples/rnnlm-0.2b/rnnlm.cpp\n",
            "./simple-examples/rnnlm-0.2b/rnnlmlib.cpp\n",
            "./simple-examples/rnnlm-0.2b/rnnlmlib.h\n",
            "./simple-examples/rnnlm-0.2b/prob.c\n",
            "./simple-examples/rnnlm-0.2b/test\n",
            "./simple-examples/rnnlm-0.2b/train\n",
            "./simple-examples/rnnlm-0.2b/valid\n",
            "./simple-examples/rnnlm-0.2b/example.sh\n",
            "./simple-examples/rnnlm-0.2b/example.output\n",
            "./simple-examples/rnnlm-0.2b/COPYRIGHT.txt\n",
            "./simple-examples/1-train/\n",
            "./simple-examples/1-train/train.sh\n",
            "./simple-examples/1-train/test.sh\n",
            "./simple-examples/1-train/README\n",
            "./simple-examples/3-combination/\n",
            "./simple-examples/3-combination/train.sh\n",
            "./simple-examples/3-combination/test.sh\n",
            "./simple-examples/3-combination/README\n",
            "./simple-examples/2-nbest-rescore/\n",
            "./simple-examples/2-nbest-rescore/lattices/\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_127040_127488.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_127513_127835.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_127865_128175.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_128188_128447.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/AMI-3E0501_u3005_128490_129032.lat.gz\n",
            "./simple-examples/2-nbest-rescore/lattices/nbest.sh\n",
            "./simple-examples/2-nbest-rescore/lattices/nbest/\n",
            "./simple-examples/2-nbest-rescore/lattices/latlist\n",
            "./simple-examples/2-nbest-rescore/README\n",
            "./simple-examples/2-nbest-rescore/getbest.c\n",
            "./simple-examples/2-nbest-rescore/gettext.c\n",
            "./simple-examples/2-nbest-rescore/makenbest.c\n",
            "./simple-examples/2-nbest-rescore/makenbest\n",
            "./simple-examples/2-nbest-rescore/gettext\n",
            "./simple-examples/2-nbest-rescore/getbest\n",
            "./simple-examples/5-one-iter/\n",
            "./simple-examples/5-one-iter/test.sh\n",
            "./simple-examples/5-one-iter/train.sh\n",
            "./simple-examples/5-one-iter/README\n",
            "./simple-examples/6-recovery-during-training/\n",
            "./simple-examples/6-recovery-during-training/test.sh\n",
            "./simple-examples/6-recovery-during-training/train.sh\n",
            "./simple-examples/6-recovery-during-training/README\n",
            "./simple-examples/7-dynamic-evaluation/\n",
            "./simple-examples/7-dynamic-evaluation/test.sh\n",
            "./simple-examples/7-dynamic-evaluation/train.sh\n",
            "./simple-examples/7-dynamic-evaluation/README\n",
            "./simple-examples/temp/\n",
            "./simple-examples/8-direct/\n",
            "./simple-examples/8-direct/train.sh\n",
            "./simple-examples/8-direct/test.sh\n",
            "./simple-examples/8-direct/README\n",
            "./simple-examples/4-data-generation/\n",
            "./simple-examples/4-data-generation/train.sh\n",
            "./simple-examples/4-data-generation/test.sh\n",
            "./simple-examples/4-data-generation/README\n",
            "./simple-examples/9-char-based-lm/\n",
            "./simple-examples/9-char-based-lm/test.sh\n",
            "./simple-examples/9-char-based-lm/train.sh\n",
            "./simple-examples/9-char-based-lm/README\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j8jci115bol2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#处理输入数据的类\n",
        "class PTBInput(object):\n",
        "  \n",
        "  def __init__(self, config, data, name=None):\n",
        "    self.batch_size = batch_size = config.batch_size\n",
        "    self.num_steps = num_steps = config.num_steps # LSTM的展开步数\n",
        "    self.epoch_size = ((len(data)//batch_size)-1) // num_steps\n",
        "    \n",
        "    self.input_data, self.targets = reader.ptb_producer(data, batch_size, num_steps, name=name) #获取特征数据input_data,以及label数据targets\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ttth8fe6bqUY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义语言模型的类\n",
        "class PTBModel(object):\n",
        "  def __init__(self, is_training, config, input_):\n",
        "    self._input = input_\n",
        "    \n",
        "    batch_size = input_.batch_size\n",
        "    num_steps = input_.num_steps\n",
        "    size = config.hidden_size\n",
        "    vocab_size = config.vocab_size\n",
        "    \n",
        "    def lstm_cell():\n",
        "      return tf.contrib.rnn.BasicLSTMCell(size, forget_bias=0.0, state_is_tuple=True)\n",
        "  \n",
        "    attn_cell = lstm_cell\n",
        "    if is_training and config.keep_prob < 1:\n",
        "      def attn_cell():\n",
        "        return tf.contrib.rnn.DropoutWrapper(lstm_cell(), output_keep_prob=config.keep_prob)\n",
        "    \n",
        "    #使用RNN的堆叠函数将前面狗找到额lstm_cell多层堆叠得到cell，堆叠次数为config中的num_layers\n",
        "    cell = tf.contrib.rnn.MultiRNNCell(\n",
        "        [attn_cell() for _ in range(config.num_layers)],\n",
        "        state_is_tuple=True\n",
        "    )\n",
        "  \n",
        "    self._initial_state = cell.zero_state(batch_size, tf.float32)\n",
        "    \n",
        "    #创建网络的词嵌入embedding部分，embedding即为将one-hot的编码格式的单词转化为向量表达形式。\n",
        "    with tf.device('/cpu:0'):\n",
        "      embedding = tf.get_variable('embedding',[vocab_size, size], dtype=tf.float32)\n",
        "      inputs = tf.nn.embedding_lookup(embedding, input_.input_data)\n",
        "      \n",
        "    if is_training and config.keep_prob<1:\n",
        "      inputs = tf.nn.dropout(inputs, config.keep_prob)\n",
        "    \n",
        "    #定义输出\n",
        "    outputs = []\n",
        "    state = self._initial_state\n",
        "    #使用variable_scope将接下来的操作的名称设为RNN\n",
        "    with tf.variable_scope('RNN'):\n",
        "      for time_step in range(num_steps):\n",
        "        if time_step>0: tf.get_variable_scope().reuse_variables()\n",
        "        (cell_output, state) = cell(inputs[:, time_step, :], state)\n",
        "        outputs.append(cell_output)\n",
        "    \n",
        "    #将output的内容用tf.concat串接起来，用reshape将其转为一个很长的一维向量。\n",
        "    output = tf.reshape(tf.concat(outputs,1),[-1,size])\n",
        "    #softmax层，先定义权重softmax_w和偏置softmax_b，然后使用tf.matmul将输出output乘上权重并加上偏置得到logits，即网络最后的输出。\n",
        "    softmax_w = tf.get_variable('softmax_w', [size, vocab_size], dtype=tf.float32)\n",
        "    softmax_b = tf.get_variable('softmax_b', [vocab_size], dtype=tf.float32)\n",
        "    \n",
        "    #得到网络的最后输出\n",
        "    logits = tf.matmul(output, softmax_w)+softmax_b\n",
        "    \n",
        "    #定义loss\n",
        "    loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example(\n",
        "        [logits],\n",
        "        [tf.reshape(input_.targets,[-1])],\n",
        "        [tf.ones([batch_size*num_steps], dtype=tf.float32)]\n",
        "    )\n",
        "    self._cost = cost = tf.reduce_sum(loss)/batch_size\n",
        "    self._final_state = state\n",
        "    \n",
        "    if not is_training:\n",
        "      return\n",
        "    \n",
        "    #定义学习速率的变量_lr,并将其设为不可训练\n",
        "    self._lr = tf.Variable(0.0, trainable=False)\n",
        "    tvars = tf.trainable_variables()\n",
        "    grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars),config.max_grad_norm)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(self._lr)\n",
        "    self._train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
        "                                              global_step=tf.contrib.framework.get_or_create_global_step())\n",
        "    \n",
        "    #设置_new_lr用以控制学习速率，同时定义操作_lr_update\n",
        "    self._new_lr = tf.placeholder(tf.float32, shape=[], name='new_learning_rate')\n",
        "    \n",
        "    #定义_lr_update，使用tf.assign将_new_lr的值赋给当前的学习速率_lr\n",
        "    self._lr_update = tf.assign(self._lr, self._new_lr)\n",
        "    \n",
        "  #定义assign_lr的函数，用来在外部控制模型的学习速率，方式是将学习速率值传入_new_lr这个placeholder,并执行update_lr操作完成对学习速率的修改\n",
        "  def assign_lr(self, session, lr_value):\n",
        "    session.run(self._lr_update, feed_dict={self._new_lr: lr_value})\n",
        "  \n",
        "  \n",
        "  #定义PTBModel类的一些property，python中的@property装饰器可以将返回变量设为只读，防止修改变量引发的问题\n",
        "  @property\n",
        "  def input(self):\n",
        "    return self._input\n",
        "  \n",
        "  @property\n",
        "  def initial_state(self):\n",
        "    return self._initial_state\n",
        "  \n",
        "  @property\n",
        "  def cost(self):\n",
        "    return self._cost\n",
        "  \n",
        "  @property\n",
        "  def final_state(self):\n",
        "    return self._final_state\n",
        "  \n",
        "  @property\n",
        "  def lr(self):\n",
        "    return self._lr\n",
        "  \n",
        "  @property\n",
        "  def train_op(self):\n",
        "    return self._train_op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9kBwlJBnbsIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#定义几种不同大小的模型的参数\n",
        "#首先是小模型的设置\n",
        "class SmallConfig(object):\n",
        "  init_scale = 0.1 #网络中权重值的初始scale\n",
        "  learning_rate = 1.0 #学习速率的初始值\n",
        "  max_grad_norm = 5 #梯度的最大范数\n",
        "  num_layers = 2 #LSTM可以堆叠的层数\n",
        "  num_steps = 20 #LSTM梯度反向传播的展开步数\n",
        "  hidden_size = 200 #LSTM内隐含节点数\n",
        "  max_epoch = 4 #初始学习速率可训练的epoch数\n",
        "  max_max_epoch = 13 #总共可训练的epoch数\n",
        "  keep_prob = 1.0 #dropout层保留节点的比例\n",
        "  lr_decay = 0.5 #学习速率衰减速度\n",
        "  batch_size = 20#每个batch中样本的数量\n",
        "  vocab_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LDJHSossbtvD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#MediumConfig中型模型\n",
        "class MediumConfig(object):\n",
        "  init_scale = 0.05 #减小了init_state,即希望权重初值不要过大，小一些有利于温和的训练\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 5\n",
        "  num_layers = 2\n",
        "  num_steps = 35 #将梯度反向传播的展开步数从20提升到35\n",
        "  hidden_size = 650 #增大约3倍\n",
        "  max_epoch = 6\n",
        "  max_max_epoch = 39 #增大到3倍\n",
        "  keep_prob = 0.5 #设置为0.5\n",
        "  lr_decay = 0.8 #衰减速率增大\n",
        "  batch_size = 20\n",
        "  vocab_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkYPC9lQbvFS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#LargeConfig大型模型\n",
        "class LargeConfig(object):\n",
        "  init_scale = 0.04 #进一步缩小了init_scale\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 10\n",
        "  num_layers = 2\n",
        "  num_steps = 35\n",
        "  hidden_size = 1500\n",
        "  max_epoch = 14\n",
        "  max_max_epoch = 55\n",
        "  heep_prob = 0.35\n",
        "  lr_decay = 1/1.15\n",
        "  batch_size = 20\n",
        "  vocab_size = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OHHIdZdQbwPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#测试用，参数都尽量使用最小值\n",
        "class TestConfig(object):\n",
        "  init_scale = 0.1\n",
        "  learning_rate = 1.0\n",
        "  max_grad_norm = 1\n",
        "  num_layers = 1\n",
        "  num_steps = 2\n",
        "  hidden_size = 2\n",
        "  max_epoch = 1\n",
        "  max_max_epoch = 1\n",
        "  keep_prob = 1.0\n",
        "  lr_decay = 0.5\n",
        "  batch_size = 20\n",
        "  vocab_size =10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S0AUYHrSbxiZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_epoch(session, model, eval_op=None, verbose=False):\n",
        "  start_time = time.time()\n",
        "  costs = 0.0\n",
        "  iters = 0\n",
        "  state = session.run(model.initial_state)\n",
        "  \n",
        "  fetches = {\n",
        "      'cost':model.cost,\n",
        "      'final_state':model.final_state,\n",
        "  }\n",
        "  \n",
        "  if eval_op is not None:\n",
        "    fetches['eval_op'] = eval_op\n",
        "    \n",
        "  for step in range(model.input.epoch_size):\n",
        "    feed_dict = {}\n",
        "    for i, (c,h) in enumerate(model.initial_state):\n",
        "      feed_dict[c] = state[i].c\n",
        "      feed_dict[h] = state[i].h\n",
        "      \n",
        "    vals = session.run(fetches, feed_dict)\n",
        "    cost = vals['cost']\n",
        "    state = vals['final_state']\n",
        "    \n",
        "    costs += cost\n",
        "    iters += model.input.num_steps\n",
        "    \n",
        "    if verbose and step % (model.input.epoch_size // 10) == 10:\n",
        "      print('%.3f perplexity: %.3f speed: %.0f wps' %\n",
        "            (step*1.0/model.input.epoch_size, np.exp(costs/iters),\n",
        "            iters*model.input.batch_size/(time.time() - start_time))\n",
        "           )\n",
        "      \n",
        "  return np.exp(costs/iters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qdg-aOg9b0Di",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "raw_data = reader.ptb_raw_data('simple-examples/data/')\n",
        "train_data, valid_data, test_data, _ = raw_data\n",
        "\n",
        "config = SmallConfig()\n",
        "eval_config = SmallConfig()\n",
        "eval_config.batch_size = 1\n",
        "eval_config.num_steps = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1iBtKHm2b1c5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3328
        },
        "outputId": "f2ae0114-81e6-4994-8a11-b217347a3f08"
      },
      "cell_type": "code",
      "source": [
        "#创建默认的graph\n",
        "with tf.Graph().as_default():\n",
        "  initializer = tf.random_uniform_initializer(-config.init_scale, config.init_scale)\n",
        "  \n",
        "  with tf.name_scope('Train'):\n",
        "    train_input = PTBInput(config=config, data=train_data, name='TrainInput')\n",
        "    with tf.variable_scope('Model', reuse=None, initializer=initializer):\n",
        "      m=PTBModel(is_training=True, config=config, input_=train_input)\n",
        "      \n",
        "  with tf.name_scope('Valid'):\n",
        "    valid_input = PTBInput(config=config, data=valid_data, name='ValidInput')\n",
        "    \n",
        "    with tf.variable_scope('Model', reuse=True, initializer=initializer):\n",
        "      mvalid = PTBModel(is_training=False, config=config,input_=valid_input)\n",
        "      \n",
        "  with tf.name_scope('Test'):\n",
        "    test_input = PTBInput(config=eval_config, data=test_data, name='TestInput')\n",
        "    with tf.variable_scope('Model', reuse=True, initializer=initializer):\n",
        "      mtest = PTBModel(is_training=False, config=eval_config, input_=test_input)\n",
        "      \n",
        "  #创建训练的管理器\n",
        "  sv = tf.train.Supervisor()\n",
        "  with sv.managed_session() as session: #创建默认的session\n",
        "    for i in range(config.max_max_epoch):\n",
        "      lr_decay = config.lr_decay ** max(i+1-config.max_max_epoch, 0.0)\n",
        "      m.assign_lr(session, config.learning_rate * lr_decay)\n",
        "      \n",
        "      print('Epoch: %d Learning rate: %.3f' % (i+1, session.run(m.lr)))\n",
        "      train_perplexity = run_epoch(session, m, eval_op=m.train_op,verbose=True)\n",
        "      print('Epoch: %d Train Perplexity: %.3f' %(i+1, train_perplexity))\n",
        "      \n",
        "      valid_perplexity = run_epoch(session, mvalid)\n",
        "      print('Epoch: %d Valid Perplexity: %.3f' % (i+1, valid_perplexity))\n",
        "      \n",
        "    test_perplexity = run_epoch(session, mtest)\n",
        "    print('Test Perplexity: %.3f' % test_perplexity)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-5-278808f8884d>:72: get_or_create_global_step (from tensorflow.contrib.framework.python.ops.variables) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.get_or_create_global_step\n",
            "WARNING:tensorflow:From <ipython-input-12-ab53a2b90b12>:21: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Starting standard services.\n",
            "WARNING:tensorflow:Standard services need a 'logdir' passed to the SessionManager\n",
            "INFO:tensorflow:Starting queue runners.\n",
            "Epoch: 1 Learning rate: 1.000\n",
            "0.004 perplexity: 5670.663 speed: 5150 wps\n",
            "0.104 perplexity: 842.954 speed: 8339 wps\n",
            "0.204 perplexity: 628.404 speed: 8577 wps\n",
            "0.304 perplexity: 509.000 speed: 8666 wps\n",
            "0.404 perplexity: 438.024 speed: 8715 wps\n",
            "0.504 perplexity: 391.639 speed: 8747 wps\n",
            "0.604 perplexity: 352.292 speed: 8766 wps\n",
            "0.703 perplexity: 325.380 speed: 8780 wps\n",
            "0.803 perplexity: 304.146 speed: 8792 wps\n",
            "0.903 perplexity: 284.757 speed: 8809 wps\n",
            "Epoch: 1 Train Perplexity: 270.245\n",
            "Epoch: 1 Valid Perplexity: 177.782\n",
            "Epoch: 2 Learning rate: 1.000\n",
            "0.004 perplexity: 212.162 speed: 9212 wps\n",
            "0.104 perplexity: 151.099 speed: 9169 wps\n",
            "0.204 perplexity: 158.688 speed: 9182 wps\n",
            "0.304 perplexity: 153.433 speed: 9176 wps\n",
            "0.404 perplexity: 150.717 speed: 9179 wps\n",
            "0.504 perplexity: 148.290 speed: 9175 wps\n",
            "0.604 perplexity: 143.582 speed: 9174 wps\n",
            "0.703 perplexity: 141.402 speed: 9167 wps\n",
            "0.803 perplexity: 139.347 speed: 9164 wps\n",
            "0.903 perplexity: 135.738 speed: 9166 wps\n",
            "Epoch: 2 Train Perplexity: 133.762\n",
            "Epoch: 2 Valid Perplexity: 143.291\n",
            "Epoch: 3 Learning rate: 1.000\n",
            "0.004 perplexity: 145.418 speed: 9168 wps\n",
            "0.104 perplexity: 105.624 speed: 9153 wps\n",
            "0.204 perplexity: 114.738 speed: 9169 wps\n",
            "0.304 perplexity: 111.715 speed: 9165 wps\n",
            "0.404 perplexity: 110.892 speed: 9173 wps\n",
            "0.504 perplexity: 110.081 speed: 9176 wps\n",
            "0.604 perplexity: 107.380 speed: 9115 wps\n",
            "0.703 perplexity: 106.694 speed: 9004 wps\n",
            "0.803 perplexity: 106.044 speed: 8976 wps\n",
            "0.903 perplexity: 103.849 speed: 8960 wps\n",
            "Epoch: 3 Train Perplexity: 102.960\n",
            "Epoch: 3 Valid Perplexity: 131.984\n",
            "Epoch: 4 Learning rate: 1.000\n",
            "0.004 perplexity: 117.776 speed: 8090 wps\n",
            "0.104 perplexity: 85.564 speed: 8812 wps\n",
            "0.204 perplexity: 94.018 speed: 8816 wps\n",
            "0.304 perplexity: 91.756 speed: 8822 wps\n",
            "0.404 perplexity: 91.519 speed: 8826 wps\n",
            "0.504 perplexity: 91.180 speed: 8834 wps\n",
            "0.604 perplexity: 89.292 speed: 8841 wps\n",
            "0.703 perplexity: 89.021 speed: 8840 wps\n",
            "0.803 perplexity: 88.772 speed: 8837 wps\n",
            "0.903 perplexity: 87.177 speed: 8837 wps\n",
            "Epoch: 4 Train Perplexity: 86.698\n",
            "Epoch: 4 Valid Perplexity: 129.296\n",
            "Epoch: 5 Learning rate: 1.000\n",
            "0.004 perplexity: 102.272 speed: 7678 wps\n",
            "0.104 perplexity: 74.168 speed: 8467 wps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "0.204 perplexity: 81.648 speed: 8571 wps\n",
            "0.304 perplexity: 79.890 speed: 8621 wps\n",
            "0.404 perplexity: 79.799 speed: 8654 wps\n",
            "0.504 perplexity: 79.700 speed: 8674 wps\n",
            "0.604 perplexity: 78.205 speed: 8683 wps\n",
            "0.703 perplexity: 78.167 speed: 8694 wps\n",
            "0.803 perplexity: 78.122 speed: 8717 wps\n",
            "0.903 perplexity: 76.840 speed: 8733 wps\n",
            "Epoch: 5 Train Perplexity: 76.568\n",
            "Epoch: 5 Valid Perplexity: 126.765\n",
            "Epoch: 6 Learning rate: 1.000\n",
            "0.004 perplexity: 90.097 speed: 9134 wps\n",
            "0.104 perplexity: 66.540 speed: 9168 wps\n",
            "0.204 perplexity: 73.419 speed: 9177 wps\n",
            "0.304 perplexity: 71.967 speed: 9179 wps\n",
            "0.404 perplexity: 71.993 speed: 9174 wps\n",
            "0.504 perplexity: 71.993 speed: 9168 wps\n",
            "0.604 perplexity: 70.774 speed: 9169 wps\n",
            "0.703 perplexity: 70.886 speed: 9179 wps\n",
            "0.803 perplexity: 70.965 speed: 9179 wps\n",
            "0.903 perplexity: 69.832 speed: 9177 wps\n",
            "Epoch: 6 Train Perplexity: 69.676\n",
            "Epoch: 6 Valid Perplexity: 127.181\n",
            "Epoch: 7 Learning rate: 1.000\n",
            "0.004 perplexity: 81.584 speed: 9103 wps\n",
            "0.104 perplexity: 61.537 speed: 9164 wps\n",
            "0.204 perplexity: 67.949 speed: 9171 wps\n",
            "0.304 perplexity: 66.548 speed: 9173 wps\n",
            "0.404 perplexity: 66.661 speed: 9169 wps\n",
            "0.504 perplexity: 66.781 speed: 9163 wps\n",
            "0.604 perplexity: 65.756 speed: 9162 wps\n",
            "0.703 perplexity: 65.904 speed: 9163 wps\n",
            "0.803 perplexity: 66.033 speed: 9164 wps\n",
            "0.903 perplexity: 65.033 speed: 9163 wps\n",
            "Epoch: 7 Train Perplexity: 64.877\n",
            "Epoch: 7 Valid Perplexity: 128.288\n",
            "Epoch: 8 Learning rate: 1.000\n",
            "0.004 perplexity: 77.632 speed: 9120 wps\n",
            "0.104 perplexity: 57.862 speed: 9193 wps\n",
            "0.204 perplexity: 63.742 speed: 9197 wps\n",
            "0.304 perplexity: 62.481 speed: 9197 wps\n",
            "0.404 perplexity: 62.617 speed: 9204 wps\n",
            "0.504 perplexity: 62.710 speed: 9200 wps\n",
            "0.604 perplexity: 61.778 speed: 9197 wps\n",
            "0.703 perplexity: 61.941 speed: 9192 wps\n",
            "0.803 perplexity: 62.074 speed: 9185 wps\n",
            "0.903 perplexity: 61.230 speed: 9183 wps\n",
            "Epoch: 8 Train Perplexity: 61.120\n",
            "Epoch: 8 Valid Perplexity: 129.493\n",
            "Epoch: 9 Learning rate: 1.000\n",
            "0.004 perplexity: 71.779 speed: 9168 wps\n",
            "0.104 perplexity: 54.625 speed: 9202 wps\n",
            "0.204 perplexity: 60.146 speed: 9194 wps\n",
            "0.304 perplexity: 58.974 speed: 9194 wps\n",
            "0.404 perplexity: 59.108 speed: 9197 wps\n",
            "0.504 perplexity: 59.249 speed: 9194 wps\n",
            "0.604 perplexity: 58.461 speed: 9192 wps\n",
            "0.703 perplexity: 58.633 speed: 9192 wps\n",
            "0.803 perplexity: 58.798 speed: 9187 wps\n",
            "0.903 perplexity: 58.016 speed: 9184 wps\n",
            "Epoch: 9 Train Perplexity: 57.927\n",
            "Epoch: 9 Valid Perplexity: 131.220\n",
            "Epoch: 10 Learning rate: 1.000\n",
            "0.004 perplexity: 68.512 speed: 9207 wps\n",
            "0.104 perplexity: 52.480 speed: 9273 wps\n",
            "0.204 perplexity: 57.701 speed: 9270 wps\n",
            "0.304 perplexity: 56.458 speed: 9275 wps\n",
            "0.404 perplexity: 56.628 speed: 9276 wps\n",
            "0.504 perplexity: 56.783 speed: 9280 wps\n",
            "0.604 perplexity: 55.998 speed: 9277 wps\n",
            "0.703 perplexity: 56.120 speed: 9276 wps\n",
            "0.803 perplexity: 56.283 speed: 9276 wps\n",
            "0.903 perplexity: 55.545 speed: 9278 wps\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10 Train Perplexity: 55.455\n",
            "Epoch: 10 Valid Perplexity: 132.874\n",
            "Epoch: 11 Learning rate: 1.000\n",
            "0.004 perplexity: 63.758 speed: 9262 wps\n",
            "0.104 perplexity: 50.051 speed: 9266 wps\n",
            "0.204 perplexity: 55.026 speed: 9273 wps\n",
            "0.304 perplexity: 54.080 speed: 9289 wps\n",
            "0.404 perplexity: 54.191 speed: 9283 wps\n",
            "0.504 perplexity: 54.388 speed: 9281 wps\n",
            "0.604 perplexity: 53.735 speed: 9281 wps\n",
            "0.703 perplexity: 53.942 speed: 9280 wps\n",
            "0.803 perplexity: 54.119 speed: 9284 wps\n",
            "0.903 perplexity: 53.465 speed: 9284 wps\n",
            "Epoch: 11 Train Perplexity: 53.406\n",
            "Epoch: 11 Valid Perplexity: 134.403\n",
            "Epoch: 12 Learning rate: 1.000\n",
            "0.004 perplexity: 62.344 speed: 9291 wps\n",
            "0.104 perplexity: 48.367 speed: 9282 wps\n",
            "0.204 perplexity: 53.064 speed: 9284 wps\n",
            "0.304 perplexity: 52.087 speed: 9279 wps\n",
            "0.404 perplexity: 52.298 speed: 9276 wps\n",
            "0.504 perplexity: 52.516 speed: 9276 wps\n",
            "0.604 perplexity: 51.880 speed: 9276 wps\n",
            "0.703 perplexity: 52.091 speed: 9278 wps\n",
            "0.803 perplexity: 52.276 speed: 9283 wps\n",
            "0.903 perplexity: 51.663 speed: 9282 wps\n",
            "Epoch: 12 Train Perplexity: 51.622\n",
            "Epoch: 12 Valid Perplexity: 136.952\n",
            "Epoch: 13 Learning rate: 1.000\n",
            "0.004 perplexity: 60.334 speed: 9411 wps\n",
            "0.104 perplexity: 46.915 speed: 9355 wps\n",
            "0.204 perplexity: 51.437 speed: 9336 wps\n",
            "0.304 perplexity: 50.556 speed: 9311 wps\n",
            "0.404 perplexity: 50.744 speed: 9302 wps\n",
            "0.504 perplexity: 50.954 speed: 9294 wps\n",
            "0.604 perplexity: 50.340 speed: 9290 wps\n",
            "0.703 perplexity: 50.515 speed: 9287 wps\n",
            "0.803 perplexity: 50.735 speed: 9287 wps\n",
            "0.903 perplexity: 50.138 speed: 9284 wps\n",
            "Epoch: 13 Train Perplexity: 50.098\n",
            "Epoch: 13 Valid Perplexity: 137.961\n",
            "Test Perplexity: 132.596\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yluP2kl8b8ns",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "在本节实现了一个基于LSTM的语言模型，LSTM在处理文本等时序数据时，LSTM可以存储状态，并依靠状态对当前的输入进行处理分析和预测。RNN和LSTM赋予了神经网络记录和存储过往信息的能力，可以模仿人类的一些简单的记忆和推理功能。\n",
        "\n",
        "## 注意力机制\n",
        "目前，注意力机制是RNN和NLP领域研究的热点。这种机制让机器可以更好的模拟人脑的功能。在图像标题生成任务中，包含注意力机制的RNN可以对某一区域的图像进行分析，并生成对应的文字描述。\n",
        "\n",
        "可阅读论文[Show,Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/pdf/1502.03044.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "bPEXPAcyb5IC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}